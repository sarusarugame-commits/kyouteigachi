import requests
from bs4 import BeautifulSoup
import time
import re
import unicodedata
import random
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# â˜…ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªç”¨ç½²åâ˜…
print("ğŸ› ï¸ LOADED: Scraper Version strict_debug_v2 (Japanese Error Mode)")

# è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 2
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

def clean_text(text):
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace("Â¥", "").replace(",", "").strip()

def get_session():
    session = requests.Session()
    retries = Retry(total=MAX_RETRIES, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(pool_connections=10, pool_maxsize=10, max_retries=retries)
    session.mount("https://", adapter)
    return session

def get_soup(session, url):
    for i in range(MAX_RETRIES):
        try:
            headers = {'User-Agent': random.choice(UA_LIST)}
            res = session.get(url, headers=headers, timeout=10)
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text or "é–‹å‚¬ä¸­æ­¢" in res.text:
                    return None
                return BeautifulSoup(res.text, 'html.parser')
            time.sleep(random.uniform(1, 2))
        except Exception:
            time.sleep(RETRY_DELAY)
    return None

def extract_payout(soup, key_text):
    try:
        tables = soup.select("table")
        for tbl in tables:
            if key_text in tbl.text:
                rows = tbl.select("tr")
                for tr in rows:
                    if key_text in tr.text:
                        tds = tr.select("td")
                        for td in tds:
                            txt = clean_text(td.text)
                            if txt.isdigit() and len(txt) >= 2 and "-" not in txt:
                                return int(txt)
    except: pass
    return 0

def scrape_race_data(session, jcd, rno, date_str):
    """ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°å³åº§ã«ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹"""
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # ãƒšãƒ¼ã‚¸å–å¾—
    soup_before = get_soup(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_before: raise FileNotFoundError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘ç›´å‰æƒ…å ±ãƒšãƒ¼ã‚¸ãªã—: {jcd}å ´ {rno}R")

    soup_list = get_soup(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_list: raise FileNotFoundError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘å‡ºèµ°è¡¨ãƒšãƒ¼ã‚¸ãªã—: {jcd}å ´ {rno}R")

    soup_res = get_soup(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")

    row = {'date': date_str, 'jcd': jcd, 'rno': rno}

    # â‘  é¢¨é€Ÿ
    wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
    if wind_elem is None: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘é¢¨é€Ÿãƒ‡ãƒ¼ã‚¿ãªã— (weather1_bodyUnitLabelData)")
    row['wind'] = float(clean_text(wind_elem.text).replace("m", "").strip())

    # â‘¡ å„è‰‡ãƒ‡ãƒ¼ã‚¿
    for i in range(1, 7):
        # å±•ç¤º
        boat_cell = soup_before.select_one(f".is-boatColor{i}")
        if boat_cell is None: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®å±•ç¤ºè¡Œãªã— (.is-boatColor{i})")
        tds = boat_cell.find_parent("tbody").select("td")
        if len(tds) <= 4: raise IndexError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®å±•ç¤ºåˆ—ä¸è¶³ len={len(tds)}")
        ex_val = clean_text(tds[4].text)
        if not ex_val: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®å±•ç¤ºã‚¿ã‚¤ãƒ ç©º")
        row[f'ex{i}'] = float(ex_val)

        # ç•ªçµ„è¡¨
        list_elem = soup_list.select_one(f".is-boatColor{i}")
        if list_elem is None: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®ç•ªçµ„è¡Œãªã—")
        list_tbody = list_elem.find_parent("tbody")
        tds_list = list_tbody.select("td")
        
        # å‹ç‡
        wr_match = re.search(r"(\d\.\d{2})", clean_text(tds_list[3].text))
        if not wr_match: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®å‹ç‡ãªã—")
        row[f'wr{i}'] = float(wr_match.group(1))
        
        f_match = re.search(r"F(\d+)", clean_text(tds_list[2].text))
        row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
        
        st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", "").replace(" ", ""))
        if not st_match: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®STãªã—")
        row[f'st{i}'] = float(st_match.group(1))
        
        mo_text = clean_text(tds_list[5].text)
        mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
        if not mo_match and len(tds_list) > 6:
            mo_text = clean_text(tds_list[6].text)
            mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
        if not mo_match: raise ValueError(f"ã€ã‚¨ãƒ©ãƒ¼ã€‘{i}å·è‰‡ã®ãƒ¢ãƒ¼ã‚¿ãƒ¼ãªã—")
        row[f'mo{i}'] = float(mo_match.group(1))

    # â‘¢ ã‚ªãƒƒã‚ºï¼ˆäºˆæ¸¬æ™‚ã¯0ã§OKï¼‰
    if soup_res:
        row['tansho'] = extract_payout(soup_res, "å˜å‹")
        row['nirentan'] = extract_payout(soup_res, "2é€£å˜")
        row['sanrentan'] = extract_payout(soup_res, "3é€£å˜")
    else:
        row['tansho'] = 0
        row['nirentan'] = 0
        row['sanrentan'] = 0

    return row
